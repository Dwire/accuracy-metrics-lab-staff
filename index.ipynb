{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Metrics Lab\n",
    "\n",
    "## Measuring Classifier Performance Metrics\n",
    "\n",
    "## Objective:\n",
    "\n",
    "* Create and interpret a Confusion Matrix.\n",
    "* Calculate Precision, and identify use cases for this metric.\n",
    "* Calculate Recall, and identify use cases for this metric.\n",
    "* Calculate Accuracy, and identify use cases for this metric.\n",
    "* Calculate F-score, and identify use cases for this metric.\n",
    "\n",
    "## Measuring Classifier Performance\n",
    "\n",
    "For this lab, we're going to focus on the different ways we can measure the performance of a classifier.  We'll focus on the following metrics:\n",
    "\n",
    "* **_Precision_**\n",
    "* **_Recall_**\n",
    "* **_Accuracy_**\n",
    "* **_F1-Score_**\n",
    "\n",
    "In order to calculate these different metrics, we'll be creating a **_Confusion Matrix_**.  This will allow us to keep track of whether the model got each question right or wrong, as well as _how_ the model got it right or wrong (more on this below).\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "We'll quickly create a Decision Tree Classifier to make predictions for a dataset we're already familiar with: The _Titanic Dataset_.  \n",
    "\n",
    "Run the cell below to import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass     Sex   Age  SibSp  Parch  \\\n",
       "0            1         0       3    male  22.0      1      0   \n",
       "1            2         1       1  female  38.0      1      0   \n",
       "2            3         1       3  female  26.0      0      0   \n",
       "3            4         1       1  female  35.0      1      0   \n",
       "4            5         0       3    male  35.0      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin Embarked  \n",
       "0         A/5 21171   7.2500   NaN        S  \n",
       "1          PC 17599  71.2833   C85        C  \n",
       "2  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3            113803  53.1000  C123        S  \n",
       "4            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "raw_df = pd.read_csv(\"titanic.csv\")\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll clean the dataset by removing unneeded columns.  We'll also binary-encode the `'Sex'` column, as well as one-hot encode the 'Embarked' column (creating a binary-encoded column for each possible category in the `'Embarked'` column).  \n",
    "\n",
    "We'll also remove null values, and then store the target column in a separate column and remove it from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex   Age  SibSp  Parch     Fare  Embarked_C  Embarked_Q  \\\n",
       "0       3    0  22.0      1      0   7.2500           0           0   \n",
       "1       1    1  38.0      1      0  71.2833           1           0   \n",
       "2       3    1  26.0      0      0   7.9250           0           0   \n",
       "3       1    1  35.0      1      0  53.1000           0           0   \n",
       "4       3    0  35.0      0      0   8.0500           0           0   \n",
       "\n",
       "   Embarked_S  \n",
       "0           1  \n",
       "1           0  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unneeded columns\n",
    "intermediate_df1 = raw_df.drop(['PassengerId', 'Ticket', 'Cabin'], axis=1, inplace=False)\n",
    "\n",
    "# Binary encode Sex column\n",
    "intermediate_df1['Sex'] = intermediate_df1['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# Remove Null values\n",
    "intermediate_df2 = intermediate_df1.dropna(inplace=False)\n",
    "\n",
    "# one-hot encode Embarked column\n",
    "intermediate_df3 = pd.get_dummies(intermediate_df2)\n",
    "\n",
    "# Store labels separately and drop from dataframe\n",
    "target = intermediate_df3['Survived']\n",
    "clean_df = intermediate_df3.drop('Survived', axis=1, inplace=False)\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Now that our data is ready, we'll split our data into training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(clean_df, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split our data into training and testing sets, we'll fit a **_Decision Tree Classifier_** and use it to make predictions on our testing set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some predictions, we can start the lab in earnest. \n",
    "\n",
    "We'll start by building a **_Confusion Matrix_**.\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "Let's start by examining the first 10 predictions from the model, and seeing how it compares to the the ground truth labels stored in `x_test`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction for 1: 1\n",
      "    actual for 1: 1\n",
      "prediction for 2: 1\n",
      "    actual for 2: 1\n",
      "prediction for 3: 0\n",
      "    actual for 3: 0\n",
      "prediction for 4: 0\n",
      "    actual for 4: 1\n",
      "prediction for 5: 1\n",
      "    actual for 5: 0\n",
      "prediction for 6: 0\n",
      "    actual for 6: 0\n",
      "prediction for 7: 1\n",
      "    actual for 7: 1\n",
      "prediction for 8: 0\n",
      "    actual for 8: 0\n",
      "prediction for 9: 1\n",
      "    actual for 9: 1\n",
      "prediction for 10: 1\n",
      "    actual for 10: 1\n"
     ]
    }
   ],
   "source": [
    "list_y_test = list(y_test)\n",
    "\n",
    "for ind, val in enumerate(preds[:10]):\n",
    "    print('prediction for {}: {}'.format(ind + 1, val))\n",
    "    print('    actual for {}: {}'.format(ind + 1, list_y_test[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four different outcomes possible here.  Let's examine each of them:\n",
    "\n",
    "* **_True Positive_**: Predicted a 1, and it's actually a 1. (Examples 1, 2, 7, 9, and 10)\n",
    "* **_True Negative_**: Predicted a 0, and it's actually a 0. (Examples 3, 6, and 8)\n",
    "* **_False Positive_**: Predicted a 1, and it's actually a 0. (Example 5)\n",
    "* **_False Negative_**: Predicted a 0, and it's actually a 1. (Example 4)\n",
    "\n",
    "A **_Confusion Matrix_** is a tally of the total numbers of each type of outcome.  Take a look at a picture of a sample confusion matrix below:\n",
    "\n",
    "<center><img src='confusion_matrix.png'></center>\n",
    "\n",
    "A python dictionary is the perfect data type for building a confusion matrix.  Below, we'll write a function that takes in the predictions and labels, and returns the confusion matrix as a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FN': 18, 'FP': 19, 'TN': 87, 'TP': 54}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def confusion_matrix(predicted, actual):\n",
    "    cm = {\n",
    "        'TP': 0,\n",
    "        'TN': 0,\n",
    "        'FP': 0,\n",
    "        'FN': 0\n",
    "    }\n",
    "    \n",
    "    for ind, val, in enumerate(list(actual)):\n",
    "        pred = predicted[ind]\n",
    "        if val == 0:\n",
    "            if pred == 0:\n",
    "                cm['TN'] += 1\n",
    "            else:\n",
    "                cm['FN'] += 1\n",
    "        else:\n",
    "            if pred == 1:\n",
    "                cm['TP'] += 1\n",
    "            else:\n",
    "                cm['FP'] += 1\n",
    "    return cm\n",
    "\n",
    "cm = confusion_matrix(preds, y_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Metrics\n",
    "\n",
    "Now that we have our confusion matrix, we can start calculating the accuracy metrics we're interested in. \n",
    "\n",
    "### Precision\n",
    "**_Precision_** measures the percentage of positive predictions that were correct.  The equation for calculating Precision is:\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "$$\n",
    "\\normalsize \n",
    "Precision = \\frac{True\\ Positives}{True\\ Positives + False\\ Positives}\n",
    "$$  \n",
    "</center>\n",
    "\n",
    "\n",
    "Write a function that takes in a confusion matrix and returns the precision score in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7397260273972602\n",
      "Expected:  0.7397260273972602\n"
     ]
    }
   ],
   "source": [
    "def precision(cm):\n",
    "    return cm['TP'] / (cm['TP'] + cm['FP'])\n",
    "\n",
    "precision_score = precision(cm)\n",
    "print('Precision: {}'.format(precision_score))\n",
    "print('Expected:  0.7397260273972602')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "**_Recall_** measures the number of percentage of positive cases that our model caught, out of all positive cases. The equation for calculating recall is:\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "$$\n",
    "\\normalsize \n",
    "Recall = \\frac{True\\ Positives}{True\\ Positives + False\\ Negatives}\n",
    "$$  \n",
    "</center>\n",
    "\n",
    "Write a function that takes in a confusion matrix and returns the recall score in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:    0.75\n",
      "Expected:  0.75\n"
     ]
    }
   ],
   "source": [
    "def recall(cm):\n",
    "    return cm['TP'] / (cm['TP'] + cm['FN'])\n",
    "\n",
    "recall_score = recall(cm)\n",
    "print('Recall:    {}'.format(recall_score))\n",
    "print('Expected:  0.75')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "**_Accuracy_** measures the percentage of predictions we got correct, out of all possible predictions.  The Equation for calculating Accuracy is:\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "$$\n",
    "\\normalsize \n",
    "Accuracy = \\frac{True\\ Positives + True\\ Negatives}{True\\ Positives + True\\ Negatives + False\\ Positives + False\\ Negatives}\n",
    "$$  \n",
    "</center>\n",
    "\n",
    "Write a function that takes in a confusion matrix and returns the Accuracy score in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:    0.7921348314606742\n",
      "Expected:    0.7921348314606742\n"
     ]
    }
   ],
   "source": [
    "def accuracy(cm):\n",
    "    return (cm['TP'] + cm['TN']) / (cm['TP'] + cm['TN'] + cm['FP'] + cm['FN'])\n",
    "\n",
    "accuracy_score = accuracy(cm)\n",
    "print('Accuracy:    {}'.format(accuracy_score))\n",
    "print('Expected:    0.7921348314606742')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score\n",
    "**_F1-Score_** is the _harmonic mean_ of precision and recall.  Because this is a weighted average of precision and recall, it is generally considered the most accurate measure of test performance.  The equation for calculating F1-Score is:\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "$$\n",
    "\\normalsize \n",
    "F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}\n",
    "$$  \n",
    "</center>\n",
    "\n",
    "Write a function that takes in a confusion matrix and returns the F1-score in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:    0.7448275862068966\n",
      "Expected:    0.7448275862068966\n"
     ]
    }
   ],
   "source": [
    "def f1_score(cm):\n",
    "    p = precision(cm)\n",
    "    r = recall(cm)\n",
    "    return 2 * p * r / (p + r)\n",
    "\n",
    "f1 = f1_score(cm)\n",
    "print('f1-score:    {}'.format(f1))\n",
    "print('Expected:    0.7448275862068966')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it--now you know how to calculate the most important accuracy metrics for classification using a confusion matrix, as well as an intuition for which metrics are most useful depending on the problem you're trying to solve!\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, we learned how to:\n",
    "* Create a **_Confusion Matrix_**\n",
    "* What **_True Positive, True Negative, False Positive,_** and **_False Negative_** mean in classification\n",
    "* What **_Precision_** is, and how to calculate it\n",
    "* What **_Recall_** is, and how to calculate it\n",
    "* What **_Accuracy_** is, and how to calculate it\n",
    "* What **_F1-Score_** is, and how to calculate it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
